{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a564f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pre-trained Enformer model to get pooled and base level contribution scores for CD69 locus**\n",
    "# code taken/adapted from Enformer authors at https://github.com/deepmind/deepmind-research/blob/master/enformer/enformer-usage.ipynb**\n",
    "# correspond to bottom track for figure 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7949db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import joblib\n",
    "import gzip\n",
    "import kipoiseq\n",
    "from kipoiseq import Interval\n",
    "import pyfaidx\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from deeplift import dinuc_shuffle\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80844cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'https://tfhub.dev/deepmind/enformer/1'\n",
    "fasta_file = '../../../reference_files/hg38.fa' ## make sure points to valid GRCh38 fasta\n",
    "#!samtools faidx /home/jupyter/reference/align_refs/GRCh38/Homo_sapiens.GRCh38.dna.primary_assembly.chr.fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b2c3e3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>genome</th>\n",
       "      <th>identifier</th>\n",
       "      <th>file</th>\n",
       "      <th>clip</th>\n",
       "      <th>scale</th>\n",
       "      <th>sum_stat</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF916JRN</td>\n",
       "      <td>/home/drk/tillage/datasets/human/dnase/encode/...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>DNASE:Jurkat clone E61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1208</th>\n",
       "      <td>1208</td>\n",
       "      <td>0</td>\n",
       "      <td>ENCFF017AFO</td>\n",
       "      <td>/home/drk/tillage/datasets/human/chip/encode/E...</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>mean</td>\n",
       "      <td>CHIP:H3K4me3:Jurkat clone E61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4831</th>\n",
       "      <td>4831</td>\n",
       "      <td>0</td>\n",
       "      <td>CNhs11253</td>\n",
       "      <td>/home/drk/tillage/datasets/human/cage/fantom/C...</td>\n",
       "      <td>384</td>\n",
       "      <td>1</td>\n",
       "      <td>sum</td>\n",
       "      <td>CAGE:acute lymphoblastic leukemia (T-ALL) cell...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  genome   identifier  \\\n",
       "120     120       0  ENCFF916JRN   \n",
       "1208   1208       0  ENCFF017AFO   \n",
       "4831   4831       0    CNhs11253   \n",
       "\n",
       "                                                   file  clip  scale sum_stat  \\\n",
       "120   /home/drk/tillage/datasets/human/dnase/encode/...    32      2     mean   \n",
       "1208  /home/drk/tillage/datasets/human/chip/encode/E...    32      2     mean   \n",
       "4831  /home/drk/tillage/datasets/human/cage/fantom/C...   384      1      sum   \n",
       "\n",
       "                                            description  \n",
       "120                              DNASE:Jurkat clone E61  \n",
       "1208                      CHIP:H3K4me3:Jurkat clone E61  \n",
       "4831  CAGE:acute lymphoblastic leukemia (T-ALL) cell...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download targets from Basenji2 dataset \n",
    "# Cite: Kelley et al Cross-species regulatory sequence activity prediction. PLoS Comput. Biol. 16, e1008050 (2020).\n",
    "targets_txt = 'https://raw.githubusercontent.com/calico/basenji/master/manuscripts/cross2020/targets_human.txt'\n",
    "df_targets = pd.read_csv(targets_txt, sep='\\t')\n",
    "df_targets[df_targets['description'].str.contains(\"Jurkat\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8831cff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LENGTH = 393216\n",
    "\n",
    "class Enformer:\n",
    "\n",
    "    def __init__(self,tfhub_url):\n",
    "        self._model = hub.load(tfhub_url).model\n",
    "\n",
    "\n",
    "    def predict_on_batch(self, inputs):\n",
    "        predictions = self._model.predict_on_batch(inputs)\n",
    "        return {k: v.numpy() for k, v in predictions.items()}\n",
    "\n",
    "    @tf.function\n",
    "    def contribution_input_grad(self, input_sequence,\n",
    "                                target_mask, track_index,\n",
    "                                output_head='human'):\n",
    "        input_sequence = input_sequence[tf.newaxis]\n",
    "\n",
    "        target_mask_mass = tf.reduce_sum(target_mask)\n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(input_sequence)\n",
    "            pred = self._model.predict_on_batch(input_sequence)[output_head][::track_index]\n",
    "            #print(pred.shape)\n",
    "            prediction = tf.reduce_sum(\n",
    "                  target_mask[tf.newaxis] * pred) / target_mask_mass\n",
    "        grad = tape.gradient(prediction, input_sequence)\n",
    "        input_grad = grad * input_sequence\n",
    "        input_grad = tf.squeeze(input_grad, axis=0)\n",
    "\n",
    "        return tf.reduce_sum(input_grad, axis=-1), grad\n",
    "    def vars_return(self):\n",
    "        return self._model.signatures#['serving_default'].variables\n",
    "\n",
    "# @title `variant_centered_sequences`\n",
    "#with strategy.scope():\n",
    "class FastaStringExtractor:\n",
    "\n",
    "    def __init__(self, fasta_file):\n",
    "        self.fasta = pyfaidx.Fasta(fasta_file)\n",
    "        self._chromosome_sizes = {k: len(v) for k, v in self.fasta.items()}\n",
    "\n",
    "    def extract(self, interval: Interval, **kwargs) -> str:\n",
    "        # Truncate interval if it extends beyond the chromosome lengths.\n",
    "        chromosome_length = self._chromosome_sizes[interval.chrom]\n",
    "        trimmed_interval = Interval(interval.chrom,\n",
    "                                    max(interval.start, 0),\n",
    "                                    min(interval.end, chromosome_length),\n",
    "                                    )\n",
    "        # pyfaidx wants a 1-based interval\n",
    "        sequence = str(self.fasta.get_seq(trimmed_interval.chrom,\n",
    "                                          trimmed_interval.start + 1,\n",
    "                                          trimmed_interval.stop).seq).upper()\n",
    "        # Fill truncated values with N's.\n",
    "        pad_upstream = 'N' * max(-interval.start, 0)\n",
    "        pad_downstream = 'N' * max(interval.end - chromosome_length, 0)\n",
    "        return pad_upstream + sequence + pad_downstream\n",
    "\n",
    "    def close(self):\n",
    "        return self.fasta.close()\n",
    "    \n",
    "def one_hot_encode(sequence):\n",
    "    return kipoiseq.transforms.functional.one_hot_dna(sequence).astype(np.float32)\n",
    "\n",
    "\n",
    "def importance_scores(chrom, start, stop, target_index, mask_indices):\n",
    "\n",
    "    target_interval = kipoiseq.Interval(chrom, int(start), int(stop))\n",
    "    resized_interval = target_interval.resize(SEQUENCE_LENGTH)\n",
    "    sequence_one_hot = one_hot_encode(fasta_extractor.extract(resized_interval))\n",
    "    predictions = model.predict_on_batch(sequence_one_hot[np.newaxis])['human'][0]\n",
    "\n",
    "    target_mask = np.zeros_like(predictions)\n",
    "    for idx in mask_indices:\n",
    "        target_mask[idx, target_index] = 1\n",
    "    # This will take some time since tf.function needs to get compiled.\n",
    "    contribution_scores, grad = model.contribution_input_grad(sequence_one_hot.astype(np.float32), target_mask, target_index)\n",
    "    contribution_scores = contribution_scores.numpy()\n",
    "    pooled_contribution_scores = tf.nn.avg_pool1d(np.abs(contribution_scores)[np.newaxis,\n",
    "                                                                              :, np.newaxis],\n",
    "                                                  128, 128, 'VALID')[0, :, 0].numpy()\n",
    "\n",
    "    base_scores = (sequence_one_hot[:][:].T * [contribution_scores[:],\n",
    "                                                   contribution_scores[:],\n",
    "                                                   contribution_scores[:],\n",
    "                                                   contribution_scores[:]]).T\n",
    "\n",
    "    gradient = np.multiply(sequence_one_hot[:][:].T, (np.squeeze(grad).T))\n",
    "    ###### dinucleotide shuffled sequences\n",
    "    seq_shuffled = dinuc_shuffle.dinuc_shuffle(sequence_one_hot, 1)[0]\n",
    "    \n",
    "    target_mask = np.zeros_like(predictions)\n",
    "    for idx in mask_indices:\n",
    "        target_mask[idx, target_index] = 1\n",
    "    # This will take some time since tf.function needs to get compiled.\n",
    "\n",
    "    contribution_scores_scram, grad_scram = model.contribution_input_grad(seq_shuffled, target_mask, target_index)\n",
    "    contribution_scores_scram = contribution_scores_scram.numpy()\n",
    "    pooled_contribution_scores_scram = tf.nn.avg_pool1d(np.abs(contribution_scores_scram)[np.newaxis,\n",
    "                                                                              :, np.newaxis],\n",
    "                                                  128, 128, 'VALID')[0, :, 0].numpy()\n",
    "\n",
    "    ## get base level matrix\n",
    "\n",
    "    base_scores_scram = (seq_shuffled[:][:].T * [contribution_scores_scram[:],\n",
    "                                        contribution_scores_scram[:],\n",
    "                                        contribution_scores_scram[:],\n",
    "                                        contribution_scores_scram[:]]).T\n",
    "\n",
    "    ## get base level matri\n",
    "    gradient_scram = np.multiply(seq_shuffled[:][:].T, (np.squeeze(grad_scram).T))\n",
    "\n",
    "\n",
    "        \n",
    "    return resized_interval,contribution_scores,pooled_contribution_scores,base_scores,np.squeeze(grad), sequence_one_hot,base_scores_scram\n",
    "\n",
    "    \n",
    "def write_out_bedgraph_pooled(pooled_contribution_scores, interval, filename_base):\n",
    "    start = interval.start\n",
    "    end =  interval.end\n",
    "    chrom = interval.chrom\n",
    "    name = '_'.join([str(chrom), str(start), str(end)])\n",
    "\n",
    "    out_file = open(filename_base + '.pooled.bedGraph', 'w')\n",
    "\n",
    "\n",
    "    for k, value in enumerate(pooled_contribution_scores):\n",
    "\n",
    "        start_interval = k * 128 + start\n",
    "        end_interval = (k+1) * 128 + start\n",
    "\n",
    "        line = [str(chrom),\n",
    "                str(start_interval), str(end_interval),\n",
    "                str(value)]\n",
    "\n",
    "        out_file.write('\\t'.join(line) + '\\n')\n",
    "    out_file.close()\n",
    "    \n",
    "def write_out_bedgraph_all(contribution_scores, interval, filename_base):\n",
    "    start = interval.start\n",
    "    end =  interval.end\n",
    "    chrom = interval.chrom\n",
    "    name = '_'.join([str(chrom), str(start), str(end)])\n",
    "\n",
    "    out_file = open(filename_base + '.all.bedGraph', 'w')\n",
    "\n",
    "\n",
    "    for k, value in enumerate(contribution_scores):\n",
    "\n",
    "        start_interval = start + k\n",
    "        end_interval = start + k + 1\n",
    "\n",
    "        line = [str(chrom),\n",
    "                str(start_interval), str(end_interval),\n",
    "                str(value)]\n",
    "\n",
    "        out_file.write('\\t'.join(line) + '\\n')\n",
    "    out_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ec5ef5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Enformer(model_path)\n",
    "fasta_extractor = FastaStringExtractor(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9dd896ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Compute contribution scores\n",
    "# our target interval is \n",
    "mask_indices=[446,447,448,449,450]\n",
    "    \n",
    "out = importance_scores(\"chr12\", 9760820, 9760903,4831, mask_indices)\n",
    "resized_int, scores, pooled, base_scores,grad, seq_one_hot,base_scores_shuff = out\n",
    "\n",
    "\n",
    "write_out_bedgraph_pooled(pooled, resized_int, 'CD69.pooled.bedGraph')\n",
    "write_out_bedgraph_all(scores, resized_int, 'CD69.all.bedGraph')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e126f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-2.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-2:m102"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
